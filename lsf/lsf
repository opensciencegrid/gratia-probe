#!/usr/bin/python

#import sys, os, stat
# import time
# import datetime  # Used for seconds->datetime conversion
#import random
#import pwd, grp
import os
#import signal
#from urlparse import urlparse

from gratia.common.Gratia import DebugPrint
#import gratia.common.GratiaWrapper as GratiaWrapper
import gratia.common.Gratia as Gratia

from gratia.common2.meter import GratiaProbe, GratiaMeter

from gratia.common2.filepinput import FileInput

from gratia.lsf.accounting import AcctFile, JobFinishEvent


def DebugPrintLevel(level, *args):
    if level <= 0:
        level_str = "CRITICAL"
    elif level >= 4:
        level_str = "DEBUG"
    else:
        level_str = ["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"][level]
    level_str = "%s - Lsf: " % level_str
    DebugPrint(level, level_str, *args)


class _LsfInputStub:
    """Stub class, needs to be defined before the regular one, to avoid NameError
    """

    # Sample Records: paths and commands have been altered
    # Use a test file for a more realistic test
    value_records = [
        ['JOB_FINISH', '7.02', '1278441653', '4325472', '8723', '33636474', '1', '1278441448', '0', '0', '1278441633', 'glow', 'normal', '', '', '', 'grid1', '', '/dev/null', '/home/32271.1294166239/stdout', '/home/glow/32271.1294166239/stderr', '1294166248.4325472', '0', '1', 'v020', '64', '23.1', '', '#! /bin/sh; echo "missing"',
         '0.129980', '0.197969', '0', '0', '-1', '0', '0', '4009', '0', '0', '0', '0', '-1', '0', '0', '0', '88', '24', '-1', '', 'default', '0', '1', '', '', '0', '7712', '110208', '', '', '', '', '0', '', '0', '', '-1', '/glow', '', '', '', '-1', ''],
        ['JOB_FINISH', '7.02', '1278441653', '4325473', '8723', '33636474', '1', '1278441449', '0', '0', '1278441633', 'glow', 'normal', '', '', '', 'grid1', '', '/dev/null', '/home/32279.1294166239/stdout', '/home/glow/32279.1294166239/stderr', '1294166249.4325473', '0', '1', 'c381', '64', '23.1', '', '#! /bin/sh; echo "missing"',
         '0.133979', '0.203968', '0', '0', '-1', '0', '0', '4009', '0', '0', '0', '0', '-1', '0', '0', '0', '90', '25', '-1', '', 'default', '0', '1', '', '', '0', '7628', '110212', '', '', '', '', '0', '', '0', '', '-1', '/glow', '', '', '', '-1', ''],
        ['JOB_FINISH', '7.02', '1278441653', '4325470', '8723', '33636474', '1', '1278441446', '0', '0', '1278441633', 'glow', 'normal', '', '', '', 'grid1', '', '/dev/null', '/home/32288.1294166239/stdout', '/home/glow/32288.1294166239/stderr', '1294166246.4325470', '0', '1', 'c152', '64', '23.1', '', '#! /bin/sh; echo "missing"',
         '0.121981', '0.203968', '0', '0', '-1', '0', '0', '4009', '0', '0', '0', '0', '-1', '0', '0', '0', '90', '24', '-1', '', 'default', '0', '1', '', '', '0', '7100', '110204', '', '', '', '', '0', '', '0', '', '-1', '/glow', '', '', '', '-1', '']
    ]

    @staticmethod
    def get_records():
        for i in _LsfInputStub.value_records:
            # return the list, will be converted in object in the caller
            yield i


class LsfInput(FileInput):
    """Get Lsf usage information from accounting file
    """

    VERSION_ATTRIBUTE = 'LsfVersion'

    def get_init_params(self):
        """Return list of parameters to read form the config file"""
        return FileInput.get_init_params(self) + [LsfInput.VERSION_ATTRIBUTE]

    def start(self, static_info):
        """open DB connection and set version form config file"""
        FileInput.start(self, static_info)
        DebugPrint(4, "Lsf start, static info: %s" % static_info)
        if LsfInput.VERSION_ATTRIBUTE in static_info:
            self._set_version_config(static_info[LsfInput.VERSION_ATTRIBUTE])

    def _start_stub(self, static_info):
        """start replacement for testing: database connection errors are trapped"""
        try:
            DebugPrintLevel(4, "Testing File existence. The probe will not use it")
            FileInput.start(self, static_info)
            if self.status_ok():
                DebugPrintLevel(4, "File OK")
            else:
                DebugPrintLevel(4, "Unable to read file")
            DebugPrintLevel(4, "Closing the file")
            self.stop()
        except:
            DebugPrint(1, "Unable to read file. The test can continue since stubs are used.")
        DebugPrint(4, "Lsf start stub, static info: %s" % static_info)
        if LsfInput.VERSION_ATTRIBUTE in static_info:
            self._set_version_config(static_info[LsfInput.VERSION_ATTRIBUTE])

    def get_version(self):
        # RPM package for lsf
        # lsid -V prints the version on stderr
        # http://www.ccs.miami.edu/hpc/lsf/7.0.6/admin/cluster_ops.html
        return self._get_version(version_command='lsid -V 2>&1')

    def get_records(self, limit=None):
        """Extract the job records
        """
        checkpoint = self.checkpoint
        new_checkpoint = None
        if checkpoint:
            # prepare for checkpoint
            new_checkpoint = (checkpoint.date(), 0)
        else:
            pass

        files = []
        if self.data_file:
            DebugPrint(4, "Parsing new Lsf records in file: %s" % self.data_file)
            files = [self.data_file]
        elif self.data_dir:
            DebugPrint(4, "Parsing new Lsf records in directory: %s" % self.data_dir)
            # Assume that all LSF accounting files are named lsb.acct*
            files = self.iter_directory(self.data_dir, filename_re="lsb.acct*")
        else:
            DebugPrint(3, "No data file or data directory were specified")
        prev_r = None
        this_r = None
        for acct_file in files:
            try:
                fh = open(acct_file)
                more_records = True
                while more_records:
                    try:
                        for r in AcctFile(fh):
                            # This allows to know if the record is the last or not
                            prev_r = this_r
                            if prev_r:
                                yield prev_r
                            this_r = r
                            if new_checkpoint is not None:
                                if new_checkpoint[0] < prev_r.eventTime:
                                    new_checkpoint = (prev_r.eventTime, prev_r.fileLineNumber)
                        more_records = False
                        if new_checkpoint is not None:
                            # update checkpoint and commit (once per file)
                            # TODO: update checkpoint and commit
                            DebugPrint(4, "Saving New Checkpoint: %s (%s)" % (new_checkpoint, type(checkpoint)))
                            # new_checkpoint should be None or datetime
                            checkpoint.set_date_transaction(new_checkpoint[0], new_checkpoint[1])
                    except ValueError, e:
                        DebugPrint(2, "Error parsing %s: %s" % (acct_file, e))
                # The very last record may be incomplete or corrupted (written partially)
                # If processing also the very last record
                if self.process_last_record:
                    if this_r:
                        # Make sure that some events were processed
                        yield this_r
                        if new_checkpoint is not None:
                            if new_checkpoint[0] < this_r.eventTime:
                                # update checkpoint and commit (once per file)
                                DebugPrint(4, "Saving New Checkpoint: %s (%s)" % (
                                    (this_r.eventTime, this_r.fileLineNumber), type(checkpoint)))
                            # new_checkpoint should be None or datetime
                            checkpoint.set_date_transaction(this_r.eventTime, this_r.fileLineNumber)
            except IOError, e:
                DebugPrint(2, "Unable to open LSF accounting file %s: %s" % (acct_file, e))
            except:
                DebugPrint(2, "Unknown error parsing the LSF accounting file: %s" % acct_file)

    def _get_records_stub(self, limit=None):
        """get_records replacement for tests: records are from a pre-filled array
        limit is ignored"""
        counter = 0
        for i in _LsfInputStub.get_records():
            r = JobFinishEvent(i, counter)
            yield r
            counter += 1

    def do_test(self):
        """Test with pre-arranged value sets
        replacing: start, get_records
        """
        # replace DB calls with stubs
        self.start = self._start_stub
        self.get_records = self._get_records_stub


####### Utility functions for LsfProbe

# TODO: how are fractional system/user times handled?
def total_seconds(td, positive=True):
    """
    Returns the total number of seconds in a time interval
    :param td: time interval (datetime.timedelta)
    :return: number of seconds (int)
    """
    # More accurate: (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6
    # Only int # of seconds is needed
    retv = long(td.seconds + td.days * 24 * 3600)
    if positive and retv < 0:
        return 0
    return retv

try:
    # string formatter is available from py 2.6
    from string import Formatter

    def strfdelta(tsec, format="P{D}DT{H}H{M}M{S}S", format_no_day="PT{H}H{M}M{S}S", format_zero="PT0S"):
        """Formatting the time duration
        Duration ISO8601 format (PnYnMnDTnHnMnS): http://en.wikipedia.org/wiki/ISO_8601
        Choosing the format P[nD]TnHnMnS where days is the total number of days (if not 0), 0 values may be omitted,
        0 duration is PT0S

        :param tsec: float, number of seconds (change to timeinterval?)
        :param fmt: Format string,  ISO 8601
        :return: Formatted time duration
        """
        if not tsec:
            # 0 or None
            return format_zero
        if 0 < tsec < 86400:
            format = format_no_day
        f = Formatter()
        d = {}
        l = {'D': 86400, 'H': 3600, 'M': 60, 'S': 1}
        k = map(lambda x: x[1], list(f.parse(format)))
        rem = long(tsec)  # py 2.7 tdelta.total_seconds())

        for i in ('D', 'H', 'M', 'S'):
            if i in k and i in l.keys():
                d[i], rem = divmod(rem, l[i])

        return f.format(format, **d)
except ImportError:
    # pre 2.6
    def strfdelta(tsec, format="P%(D)dDT%(H)dH%(M)dM%(S)dS", format_no_day="P%(D)dDT%(H)dH%(M)dM%(S)dS", format_zero="PT0S"):
        """Formatting the time duration
        Duration ISO8601 format (PnYnMnDTnHnMnS): http://en.wikipedia.org/wiki/ISO_8601
        Choosing the format P[nD]TnHnMnS where days is the total number of days (if not 0), 0 values may be omitted,
        0 duration is PT0S

        :param tsec: float, number of seconds
        :param fmt: Format string,  ISO 8601
        :return: Formatted time duration
        """
        if not tsec:
            # 0 or None
            return format_zero
        d = {}
        l = {'D': 86400, 'H': 3600, 'M': 60, 'S': 1}
        rem = long(tsec)

        for i in ('D', 'H', 'M'):  # needed because keys are not sorted
            d[i], rem = divmod(rem, l[i])
        d['S'] = rem
        if d['D'] == 0:
            format = format_no_day

        return format % d


class LsfProbe(GratiaMeter):

    # Set pre-defined values
    PROBE_NAME = 'lsf'
    # dCache, xrootd, Enstore
    CE_NAME = 'LsfCE'
    # Production
    CE_STATUS = 'Production'

    def __init__(self):
        GratiaMeter.__init__(self, self.PROBE_NAME)
        self._probeinput = LsfInput()
        self._lsf_bindir = ""

    # Functions to process and send
    def complete_and_send(self, jrecord):
        r = self.lsf_to_user_record(jrecord)
        if jrecord.execHosts:
            lshost = "lshost"
            if self._lsf_bindir:
                lshost = os.path.join(self._lsf_bindir, lshost)
            self.add_hostinfo(r, jrecord, lshost)

        DebugPrint(4, "Sending LSF record for: %s" % jrecord.jobID)
        print r
        Gratia.Send(r)


    @staticmethod
    def lsf_to_user_record(jrecord):
        """
        preparing and sending a job record
        :param jrecord:
        :return:
        """
        DebugPrint(5, "Processing job record: %s" % (jrecord))

        # Empty usage record
        resource_type = "Batch"
        # resource_type = "GridMonitor"
        # resource_type = 'BatchPilot'
        r = Gratia.UsageRecord(resource_type)
        r.Grid("Local")

        # Adding ID
        r.LocalJobId(str(jrecord.jobID))
        # TODO: global id?
        # TODO: is the ID OK? Unique enough? (jrecord.fromHost, jrecord.jobID,) get_hostname?
        global_id = "%s-%s" % (jrecord.fromHost, jrecord.jobID )
        r.GlobalJobId(global_id)

        # It is actually a user name
        r.LocalUserId(jrecord.userName)

        """
        # Other data
        r.VOName(mrecord['storage_group'])
        start = GratiaProbe.parse_date(mrecord['mount_start'])
        if not finish:
            finish = start
            duration = 0
        else:
            finish = GratiaProbe.parse_date(finish)
            duration = int(float(finish)-float(start))
        r.WallDuration(duration)
        r.StartTime(GratiaProbe.format_date(start))
        """
        # r.VOName still missing


        r.EndTime(GratiaProbe.format_date(jrecord.eventTime))
        # jrecord.creationTime (submit/creation time)
        # TODO: add domain to submit host (server) - self._normalize_hostname (using DefaultDomainName from config)
        r.SubmitHost(jrecord.fromHost)
        r.Queue(jrecord.queue)
        # user above
        # jobID above
        jrecord.numProcessors  #processors
        #if jrecord.startTimeEpoch:  # used as flag to see if the job ran
        #    walltime = int(jrecord.eventTimeEpoch - jrecord.startTimeEpoch)
        if jrecord.startTimeEpoch >= 1:
            # the job started (and ran)
            r.WallDuration(strfdelta(total_seconds(jrecord.runTime)))  # python 2.7 jrecord.runTime.total_seconds
            # previous probe was reporting total (user+system)
            r.CpuDuration(total_seconds(jrecord.stime), "system", "Was entered in seconds")
            r.CpuDuration(total_seconds(jrecord.utime), "user", "Was entered in seconds")
            # mem jrecord.maxRMem
            # swap jrecord.maxRSwap
            r.StartTime(GratiaProbe.format_date(jrecord.startTime))
        # end jrecord.eventTime(Epoch)
        r.TimeInstant(GratiaProbe.format_date(jrecord.submitTime), 'SubmitTime')  # ctime
        #if jrecord.execHosts:
        #    # last exec host
        #    r.Host(jrecord.execHosts[-1])
        jrecord.jobName  #jobName
        jrecord.command  # command
        jrecord.exitStatus  # exitStatus
        return r

    def add_hostinfo(self, r, jrecord, lshost):
        if jrecord.execHosts:
            # last exec host
            r.Host(jrecord.execHosts[-1])
            if not jrecord.numProcessors or jrecord.numProcessors == 1:
                # Execute: lshosts $execHost   Host
                # which returns
                #HOST_NAME      type    model  cpuf ncpus maxmem maxswp server RESOURCES
                #c485         X86_64   PC1133  23.1     8 16046M 16378M    Yes (mpich2)
                """ perl code to imitate:
         # print "Will execute lshost $urAcctlogInfo{execHost}\n";
         my $lshosts = $lsfBinDir."/lshosts";
         open FH, "$lshosts $urAcctlogInfo{execHost} |" or die "Failed to open pipeline to/from lshost";
         my @lines = <FH>;
         close(FH);
         print "Need: @lines\n";
         if ( scalar @lines == 2) {
            my @headers = split(/ +/,$lines[0]);
            my @values = split(/ +/,$lines[1]);
            my $search = "ncpus";
            my $index = first { $headers[$_] eq $search } 0 .. $#headers;
            # print "Found value for $search = $values[$index]\n";
            $urAcctlogInfo{processors} = $values[$index] || 1;
         }
                """

                # execute lshost jrecord.execHosts[-1]
                cmd = "%s %s" % (lshost, jrecord.execHosts[-1])
                out = GratiaProbe.run_command(cmd)
                ncpus = 1
                if out:
                    rows = out.split('\n')
                    if len(rows) == 2:
                        counter = 0
                        for i in rows[0].split():
                            if i == 'ncpus':
                                ncpus = rows[1].split()[counter]
                            counter += 1
                r.Processors(ncpus, metric="max")   # max or average or total (default)

    @staticmethod
    def _is_ok(jrecord):
        return True

    def main(self):
        # Initialize the probe an the input
        self.start()
        DebugPrintLevel(4, "LSF probe started - main loop")

        #se = self.get_sitename()
        #name = self.get_probename()
        #hostname = self.get_hostname()

        jobs = {}

        # Loop over job records
        for jrecord in self._probeinput.get_records():
            """
            Job record is an accounting.JobFinishEvent object
version - LSF Version number as a string
eventTimeEpoch - time generated (= end time) in seconds since epoch
eventTime - time generated (= end time) as datetime
jobID - LSF job id
userId - numeric user ID of the user who owned the job
submitTimeEpoch, submitTime - job submitted
beginTimeEpoch, beginTime - scheduled job start time
termTimeEpoch, termTime - Job termination deadline
startTimeEpoch, startTime - start time
userName - user name of the submitter
queue - job queue
fromHost
cwd
inFile
outFile
errFile
jobFile
numAskedHosts - Number of host names to which job dispatching will be limited
askedHosts - List of host names to which job dispatching will be limited
numExHosts - Number of processors used for execution (if LSF_HPC_EXTENSIONS="SHORT_EVENTFILE" in lsf.conf, number of .hosts listed in execHosts)
execHosts - List of execution host names (allocation at job finish time)
jStatus - Job status. 32=EXIT, 64=DONE
hostFactor - CPU factor of the first execution host
jobName - Job name (up to 4094 characters)
command - Complete batch job command
# -1=unavailable, units are sec or KB
utime - User time used in seconds
stime - System time used in seconds
exutime - Exact user time used
mailUser - Name of the user to whom job related mail was sent
projectName - LSF project name
exitStatus - UNIX exit status of the job
maxNumProcessors - Maximum number of processors specified for the job
loginShell - Login shell used for the job
timeEvent
termInfo - TermInfo object
warningAction
warningTimePeriod
chargedSAAP - Share Attribute Account Path (SAAP) that was charged for the job
runTime - tun time (0 if never started, startTime-eventTime) datetime.timedelta
# Job never started
waitTime=eventTime-submitTime
startTime=termTime
# else:
waitTime=startTime-submitTime
pendTime=waitTime - The time the job was pending
            """
            DebugPrint(5, "Preparing job record for: %s" % jrecord)

            if self._is_ok(jrecord):
                # TODO: send only completed jobs?
                # also jobs that did not start?
                self.complete_and_send(jrecord)
                #del mounts[t_volume]
            else:
                # Invalid job!
                # Add also some message why it is not valid
                DebugPrint(3, "Skipping Invalid job %s." % jrecord)


if __name__ == "__main__":
    # Do the work
    LsfProbe().main()

